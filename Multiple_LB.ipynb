{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SpicyHabanero/GraphGainz/blob/main/Multiple_LB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "terminal-export"
      },
      "source": [
        "## Install dependencies"
      ],
      "id": "terminal-export"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "expressed-executive",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bb461e1-6c19-4c87-ebac-66c68160df0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m808.0/808.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llmsherpa\n",
            "  Downloading llmsherpa-0.1.3-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from llmsherpa) (1.26.18)\n",
            "Installing collected packages: llmsherpa\n",
            "Successfully installed llmsherpa-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU qdrant-client #==1.2.0!pip install torch\n",
        "!pip -q install InstructorEmbedding\n",
        "!pip -q install -U sentence-transformers\n",
        "!pip -q install anthropic\n",
        "!pip install llmsherpa"
      ],
      "id": "expressed-executive"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b3a1f93-1fbe-4ebe-8282-6e9d96349796"
      },
      "source": [
        "### Import libraries"
      ],
      "id": "1b3a1f93-1fbe-4ebe-8282-6e9d96349796"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "849c4e3b-af29-479e-b868-221586050422"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from glob import glob\n",
        "from qdrant_client import QdrantClient, models\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from anthropic import Anthropic, HUMAN_PROMPT, AI_PROMPT\n",
        "from llmsherpa.readers import LayoutPDFReader"
      ],
      "id": "849c4e3b-af29-479e-b868-221586050422"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storage Bucket"
      ],
      "metadata": {
        "id": "AaN2o8_gJO9X"
      },
      "id": "AaN2o8_gJO9X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkR4Yr2_q2L0"
      },
      "outputs": [],
      "source": [
        "pdf_directory = \"/content/\"\n",
        "pdf_files = glob(os.path.join(pdf_directory, \"*.pdf\"))"
      ],
      "id": "BkR4Yr2_q2L0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse files & Save to Dictionary"
      ],
      "metadata": {
        "id": "X4Xbm33MPW9J"
      },
      "id": "X4Xbm33MPW9J"
    },
    {
      "cell_type": "code",
      "source": [
        "llmsherpa_api_url = \"https://readers.llmsherpa.com/api/document/developer/parseDocument?renderFormat=all\"\n",
        "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
        "\n",
        "if pdf_files:\n",
        "    papers_dict = {}  # Dictionary to store papers with formatted names as keys\n",
        "    for index, pdf_url in enumerate(pdf_files, start=1):\n",
        "        formatted_name = f\"Doc_{index}\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        papers_dict[formatted_name] = pdf_reader.read_pdf(pdf_url)\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Time taken: {elapsed_time} seconds\")\n",
        "        # Do something with 'papers_dict[formatted_name]' if needed\n",
        "else:\n",
        "    print(\"No PDF files found in the specified directory.\")\n"
      ],
      "metadata": {
        "id": "8Lb_vLP7Ky9H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82e818d-efca-46a0-fcb9-ea23831bc129"
      },
      "id": "8Lb_vLP7Ky9H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.5452120304107666 seconds\n",
            "Time taken: 4.606874227523804 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Chunk based on layout"
      ],
      "metadata": {
        "id": "39p0NIoTR5JU"
      },
      "id": "39p0NIoTR5JU"
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_block_data(block):\n",
        "    return {\n",
        "        # 'tag': block.tag,\n",
        "        'content': block.to_text(),\n",
        "    }\n",
        "\n",
        "def extract_section_data(section, section_label):\n",
        "    return {\n",
        "        'label': section_label,\n",
        "        'title': section.title,\n",
        "        # 'blocks': [extract_block_data(block) for block in section.children],\n",
        "        'chunks': [extract_block_data(chunk) for chunk in section.chunks()],\n",
        "    }"
      ],
      "metadata": {
        "id": "NDDKyoRIR2fy"
      },
      "id": "NDDKyoRIR2fy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parses through each document and stores in dictionary"
      ],
      "metadata": {
        "id": "gE67HRR4NtUn"
      },
      "id": "gE67HRR4NtUn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store section data for each document\n",
        "sections_data_dict = {}\n",
        "\n",
        "# Iterate through each document in 'papers_dict'\n",
        "for document_name, document_content in papers_dict.items():\n",
        "    # Assuming 'papers' is the object that contains your PDF data\n",
        "    papers = document_content\n",
        "\n",
        "    # List to store section data for the current document\n",
        "    sections_data = []\n",
        "\n",
        "    # Iterate through sections in the document\n",
        "    for i, section in enumerate(papers.sections()):\n",
        "        # Define a simple label for each section\n",
        "        section_label = f'Section {i + 1}'\n",
        "\n",
        "        # Extract section data and append to list\n",
        "        section_data = extract_section_data(section, section_label)\n",
        "        sections_data.append(section_data)\n",
        "\n",
        "    # Assign the sections data for the current document to the dictionary\n",
        "    sections_data_dict[document_name] = sections_data"
      ],
      "metadata": {
        "id": "IJYGBhA_Tjf1"
      },
      "id": "IJYGBhA_Tjf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "092d1e71"
      },
      "outputs": [],
      "source": [
        "client = QdrantClient(\":memory:\")"
      ],
      "id": "092d1e71"
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store collection names for each document\n",
        "document_collection_names = {}\n",
        "\n",
        "# Iterate through each document in 'papers_dict'\n",
        "for document_name in papers_dict.keys():\n",
        "    print(f\"Document Name: {document_name}\")\n",
        "\n",
        "    # Fetch collections\n",
        "    collections = client.get_collections()\n",
        "\n",
        "    # Check if the current collection name exists\n",
        "    if document_name not in [c.name for c in collections.collections]:\n",
        "        print(f\"Collection Name: {document_name} does not exist.\")\n",
        "        client.recreate_collection(\n",
        "          collection_name=document_name,\n",
        "          vectors_config=models.VectorParams(\n",
        "          size=768,\n",
        "          distance=models.Distance.COSINE,\n",
        "          ),\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Collection Name: {document_name} already exists.\")\n",
        "\n",
        "    print(\"-\" * 30)  # Separator for clarity\n"
      ],
      "metadata": {
        "id": "8sER-Q3Z4e6G"
      },
      "id": "8sER-Q3Z4e6G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31a85bb3"
      },
      "outputs": [],
      "source": [
        "# set device to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load the retriever model from huggingface model hub\n",
        "retriever = INSTRUCTOR(\"hkunlp/instructor-xl\", device=device)\n",
        "retriever"
      ],
      "id": "31a85bb3"
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store dataframes for each document\n",
        "dataframes_list = []\n",
        "# Dictionary to store dataframes for each document\n",
        "dataframes_dict = {}\n",
        "\n",
        "collections = client.get_collections()\n",
        "\n",
        "# Iterate through each document in 'sections_data_dict'\n",
        "for document_name, sections_data in sections_data_dict.items():\n",
        "    # Convert the list of dictionaries to a DataFrame for each document\n",
        "    df = pd.DataFrame(sections_data, columns=['label', 'title', 'chunks'])\n",
        "\n",
        "    # Assuming df is your DataFrame with the 'chunks' column\n",
        "    df['chunks'] = df['chunks'].apply(lambda chunks: [chunk.get('content', '') for chunk in chunks])\n",
        "\n",
        "    # Remove square brackets around the chunks\n",
        "    df['chunks'] = df['chunks'].apply(lambda chunks: ', '.join(chunks))\n",
        "\n",
        "    # Add a column for document name\n",
        "    df['document_name'] = document_name\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    dataframes_list.append(df)\n",
        "\n",
        "    # Store the DataFrame in the dictionary with document_name as the key\n",
        "    dataframes_dict[document_name] = df\n",
        "\n",
        "    # Assuming df is your DataFrame with the 'label', 'title', and 'chunks' columns\n",
        "    for index, chunk in df.iterrows():\n",
        "      # Generate embeddings for the title\n",
        "      title_emb = retriever.encode([chunk['title']])[0].tolist()\n",
        "\n",
        "      # Get metadata\n",
        "      meta = chunk.to_dict()\n",
        "\n",
        "      # Create unique ID\n",
        "      current_id = index\n",
        "\n",
        "      # Upsert title vector to Qdrant\n",
        "      client.upsert(\n",
        "        collection_name=document_name,\n",
        "        points=models.Batch(ids=[current_id], vectors=[title_emb], payloads=[meta]),\n",
        "      )\n"
      ],
      "metadata": {
        "id": "fIA-STWq0eGh"
      },
      "id": "fIA-STWq0eGh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for document_name in papers_dict.keys():\n",
        "    print(f\"Document Name: {document_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM_hNs-1V_mO",
        "outputId": "5b641ba2-4581-436e-f03f-ef70268bea81"
      },
      "id": "zM_hNs-1V_mO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Name: Doc_1\n",
            "Document Name: Doc_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Display the DataFrames\n",
        "for document_name, dataframe in dataframes_dict.items():\n",
        "    print(f\"DataFrame for {document_name}:\")\n",
        "    print(dataframe)\n",
        "    print(\"-\" * 30)  # Separator for clarity"
      ],
      "metadata": {
        "id": "obuMMuxkOItD"
      },
      "id": "obuMMuxkOItD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyYaY3QEQiHZ"
      },
      "outputs": [],
      "source": [
        "def get_relevant_plot(question: str, top_k: int) -> pd.DataFrame:\n",
        "\n",
        "    try:\n",
        "        encoded_query = retriever.encode(question).tolist()  # generate embeddings for the question\n",
        "\n",
        "        # find a section in the document by title\n",
        "        result = client.search(\n",
        "          collection_name=collection_name,\n",
        "          query_vector=encoded_query,\n",
        "          limit=top_k,\n",
        "          )  # search qdrant collection for context passage with the answer\n",
        "\n",
        "        context_data = [{\"title\": x.payload[\"title\"], \"chunks\": x.payload[\"chunks\"]} for x in result]\n",
        "\n",
        "        context_df = pd.DataFrame(context_data)\n",
        "\n",
        "        return context_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print({e})"
      ],
      "id": "lyYaY3QEQiHZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Have to fix below to work with multiple constellations"
      ],
      "metadata": {
        "id": "BcoIoMhtIlJL"
      },
      "id": "BcoIoMhtIlJL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg9XTDkIJzH_"
      },
      "outputs": [],
      "source": [
        "anthropic = Anthropic(\n",
        "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_key=\"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\",\n",
        ")\n"
      ],
      "id": "hg9XTDkIJzH_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dc9VYOiUQA7B"
      },
      "outputs": [],
      "source": [
        "def extract_answer(question: str, context: pd.DataFrame) -> pd.DataFrame:\n",
        "\n",
        "    HUMAN_PROMPT =  \"\\n\\nHuman: You are a corporate credit financial analyst. You have been given documents to do due dilegence on corporate debt. Craft factual responses by being straight to the point.\"\n",
        "    AI_PROMPT = \"\\n\\nAssistant:\"\n",
        "\n",
        "    s_context = context.to_string()\n",
        "\n",
        "    best_answer = anthropic.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        max_tokens_to_sample=300,\n",
        "        prompt=f\"{HUMAN_PROMPT} <question>{question}<question> <context>{s_context}<context> {AI_PROMPT}\",\n",
        "        )\n",
        "\n",
        "    return best_answer.completion\n"
      ],
      "id": "Dc9VYOiUQA7B"
    },
    {
      "cell_type": "code",
      "source": [
        "def best_answer(question: str, context: pd.DataFrame):\n",
        "\n",
        "    HUMAN_PROMPT =  \"\\n\\nHuman: You are manager of an analyst that has answered questions using a document. Your job is to check the work of the analyst and pick the best answer to the question.\"\n",
        "    AI_PROMPT = \"\\n\\nAssistant: \"\n",
        "\n",
        "    s_context = context.to_string()\n",
        "\n",
        "    best_answer = anthropic.completions.create(\n",
        "        model=\"claude-2\",\n",
        "        max_tokens_to_sample=300,\n",
        "        prompt=f\"{HUMAN_PROMPT} <question>{question}<question> <context>{s_context}<context> {AI_PROMPT}\",\n",
        "        )\n",
        "\n",
        "    return best_answer.completion"
      ],
      "metadata": {
        "id": "zRwPKVmjtlF1"
      },
      "id": "zRwPKVmjtlF1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "\n",
        "  question = input(\"Ask a question (or press enter to quit):\")\n",
        "\n",
        "  if not question:\n",
        "    break\n",
        "\n",
        "  context = get_relevant_plot(question, top_k=3)\n",
        "  extract_answer(question, context)\n",
        "  b_answer = best_answer(question, context)\n",
        "\n",
        "\n",
        "  print(b_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COK25K724P_R",
        "outputId": "59bea7ab-2f93-4154-8b78-62d8ab32a546"
      },
      "id": "COK25K724P_R",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question (or press enter to quit):Can you please provide examples of affirmative covenants?\n",
            " Based on the context provided, here are some examples of affirmative covenants from the document:\n",
            "\n",
            "1. The Borrower will furnish various financial statements and other information to the Administrative Agent (Section 5.01). This includes annual audited financial statements.\n",
            "\n",
            "2. The Borrower will provide notice to the Administrative Agent of any material events such as litigation, ERISA events, material changes in accounting or financial reporting practices, etc (Section 5.02). \n",
            "\n",
            "3. The Borrower will provide prompt notice of any changes in loan party legal names, jurisdictions of organization, organizational IDs required for UCC filings (Section 5.03).\n",
            "\n",
            "4. The Borrower and Restricted Subsidiaries will maintain legal existence and material rights and privileges (Section 5.04). \n",
            "\n",
            "5. The Borrower and Restricted Subsidiaries will comply with laws including ERISA, Environmental Laws and PATRIOT Act (Section 5.09).\n",
            "\n",
            "6. The Borrower will use loan proceeds as per Section 3.17 (presumably described elsewhere) (Section 5.10).\n",
            "\n",
            "7. The Borrower and Restricted Subsidiaries will grant security interests and take other steps to perfect first priority liens on Collateral (Section 5.11).\n",
            "\n",
            "So in summary, key affirmative covenants cover providing ongoing financial information, notices of material events, maintaining legal status, complying with laws, use of proceeds\n",
            "Ask a question (or press enter to quit):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3722ca75-ebdf-4c2a-875a-48c7da9a66aa"
      },
      "outputs": [],
      "source": [
        "for collection in collections.collections:\n",
        "  client.delete_collection(collection_name=collection_name)"
      ],
      "id": "3722ca75-ebdf-4c2a-875a-48c7da9a66aa"
    },
    {
      "cell_type": "code",
      "source": [
        "target_document_name = \"Doc_2\"  # Use above output to print doc stuff out\n",
        "\n",
        "# Check if the target document name exists in the dictionary\n",
        "if target_document_name in sections_data_dict:\n",
        "    print(f\"Document Name: {target_document_name}\")\n",
        "\n",
        "    # Iterate through sections data for the target document\n",
        "    for section_data in sections_data_dict[target_document_name]:\n",
        "        print(f\"Label: {section_data['label']}\")\n",
        "        print(f\"Title: {section_data['title']}\")\n",
        "        print(\"Chunks:\")\n",
        "\n",
        "        # Iterate through chunks data for the current section\n",
        "        for chunk_data in section_data['chunks']:\n",
        "            print(f\"Content: {chunk_data['content']}\")\n",
        "\n",
        "        print(\"-\" * 30)  # Just a separator for clarity\n",
        "\n",
        "else:\n",
        "    print(f\"Document '{target_document_name}' not found.\")"
      ],
      "metadata": {
        "id": "UM_J2GMUTyeK"
      },
      "id": "UM_J2GMUTyeK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Assuming df is your DataFrame with the 'label', 'title', and 'chunks' columns\n",
        "for index, chunk in df.iterrows():\n",
        "    # Generate embeddings for the title\n",
        "    title_emb = retriever.encode([chunk['title']])[0].tolist()\n",
        "\n",
        "    # Get metadata\n",
        "    meta = chunk.to_dict()\n",
        "\n",
        "    # Create unique ID\n",
        "    current_id = index\n",
        "\n",
        "    # Upsert title vector to Qdrant\n",
        "    client.upsert(\n",
        "        collection_name=collection_name,\n",
        "        points=models.Batch(ids=[current_id], vectors=[title_emb], payloads=[meta]),\n",
        "    )\n",
        "\n",
        "\n",
        "collection_vector_count = client.get_collection(collection_name=collection_name).vectors_count\n",
        "print(f\"Vector count in collection: {collection_vector_count}\")\n",
        "assert collection_vector_count == len(df)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "qTJv_LzmsUa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5849172-888e-41ed-9773-1b4804e46201"
      },
      "id": "qTJv_LzmsUa8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector count in collection: 169\n",
            "Time taken: 7.7781007289886475 seconds\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-3.m65",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 333.240754,
      "end_time": "2021-04-15T21:12:11.363566",
      "environment_variables": {},
      "exception": null,
      "input_path": "/notebooks/question_answering/question_answering.ipynb",
      "output_path": "/notebooks/tmp/question_answering/question_answering.ipynb",
      "parameters": {},
      "start_time": "2021-04-15T21:06:38.122812",
      "version": "2.3.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "5fe10bf018ef3e697f9035d60bf60847932a12bface18908407fd371fe880db9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}